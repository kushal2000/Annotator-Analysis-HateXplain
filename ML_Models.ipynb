{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_Models.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"sk2RF6PCO2l8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615568062548,"user_tz":-330,"elapsed":86512,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}},"outputId":"8fd455d7-181e-4368-f876-e1e34bf264e4"},"source":["import itertools\n","import spacy\n","import random\n","import os\n","from spacy.util import minibatch, compounding\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount = True)\n","root_path = 'gdrive/My Drive/AI&Ethics/'\n","os.chdir(root_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BSBMOPfjeQ1Y"},"source":["!pip install ekphrasis\n","!pip install vaderSentiment\n","!pip install textstat\n","!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5qD-0Zy9eUL_"},"source":["import numpy as np\n","import pandas as pd\n","import pickle\n","import sys\n","from transformers import AutoModel, AutoTokenizer\n","import torch.nn as nn\n","import torch\n","import copy\n","from transformers import BertModel, RobertaModel, BertTokenizer, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, random_split, DataLoader, IterableDataset, ConcatDataset\n","import sklearn\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import f1_score, accuracy_score\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import random\n","import pickle\n","import json\n","from sklearn.metrics import accuracy_score\n","from ekphrasis.classes.preprocessor import TextPreProcessor\n","from ekphrasis.classes.tokenizer import SocialTokenizer\n","from ekphrasis.dicts.emoticons import emoticons\n","import re\n","plt.rcParams['figure.figsize'] = [15, 8]\n","plt.rcParams.update({'font.size': 8})\n","RANDOM_SEED = 42\n","model_path = 'bert-base-uncased'\n","# model_path = 'monsoon-nlp/tamillion'\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XV73bdiKRfKb"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"ZTPhX5lZpNfi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615564723024,"user_tz":-330,"elapsed":14301,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"15395307286796798856"}},"outputId":"798ee700-08e3-447e-b08c-23f446763658"},"source":["from datasets import list_datasets, load_dataset\n","dataset = load_dataset('hatexplain', split = 'train')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reusing dataset hatexplain (/root/.cache/huggingface/datasets/hatexplain/plain_text/1.0.0/802fcd855438812094e336cea509c99b04b890e4e0846c0385877ee2c7361e93)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0naFtghsHop","executionInfo":{"status":"ok","timestamp":1615564732838,"user_tz":-330,"elapsed":21990,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"15395307286796798856"}},"outputId":"92932b13-5b10-4bfa-b6bd-7d35241ac0cf"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n","  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"],"name":"stderr"},{"output_type":"stream","text":["Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n","  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"],"name":"stderr"},{"output_type":"stream","text":["Reading english - 1grams ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"miCbNC-Vf0kr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615564732839,"user_tz":-330,"elapsed":19700,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"15395307286796798856"}},"outputId":"b6023802-a840-4207-c6dd-27b6951e0d45"},"source":["def get_ids(criteria):\n","    if criteria == 'Race':\n","        column = 2\n","    elif criteria == 'Country':\n","        column = 3\n","    elif criteria == 'Age':\n","        column = 4\n","    elif criteria == 'Gender':\n","        column = 5\n","    elif criteria == 'Religion':\n","        column = 6\n","    \n","    ids = {}\n","    with open(annotator_file) as annotfile:\n","        next(annotfile)\n","        annots = annotfile.read().splitlines()\n","        for a in annots:\n","            annot = a.split(\",\")\n","            try:\n","                ids[annot[column]].append(annot[1])\n","            except KeyError:\n","                ids[annot[column]] = [annot[1]]\n","    return ids\n","\n","def get_freq(id, ids):\n","    for cl in ids.keys():\n","        if str(id) in ids[cl]:\n","            return len(ids[cl])\n","    return 200\n","\n","def get_annotator_class(id, ids):\n","    for cl in ids.keys():\n","        if str(id) in ids[cl]:\n","            return cl\n","    return None\n","\n","ids = get_ids('Religion')\n","print(type(ids))\n","# print(ids_1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'dict'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpnUuvd3r3-R","executionInfo":{"status":"ok","timestamp":1615564823348,"user_tz":-330,"elapsed":1674,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"15395307286796798856"}},"outputId":"10f86ae2-8b46-472c-dc8b-b7e89405b039"},"source":["data_file = 'data/greedy_approach_annotators.json'\n","\n","df = pd.read_json(data_file)\n","print(df.head())\n","print(len(dataset), len(df))\n","classes = ['Race', 'Country', 'Religion', 'Gender']\n","for class_name in classes:\n","    ids = get_ids(class_name)\n","    info_dict = {}\n","    for key in ids:\n","        for val in ids[key]:\n","            info_dict[val] = key\n","    class_data = []\n","    for id in df['annotator_id']: class_data.append(info_dict[str(id)])\n","    df[class_name] = class_data\n","print(df['text'])\n","df.to_csv('data/annotators_final.csv', index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   label  ...                                               text\n","0      2  ...  [u, really, think, i, would, not, have, been, ...\n","1      2  ...  [if, english, is, not, imposition, then, hindi...\n","2      2  ...  [no, liberal, congratulated, hindu, refugees, ...\n","3      2  ...  [he, said, bro, even, your, texts, sound, redn...\n","4      1  ...  [i, live, and, work, with, many, legal, mexica...\n","\n","[5 rows x 4 columns]\n","15383 14242\n","0        [u, really, think, i, would, not, have, been, ...\n","1        [if, english, is, not, imposition, then, hindi...\n","2        [no, liberal, congratulated, hindu, refugees, ...\n","3        [he, said, bro, even, your, texts, sound, redn...\n","4        [i, live, and, work, with, many, legal, mexica...\n","                               ...                        \n","14237    [thanks, for, coming, to, my, ted, talk, p.s.,...\n","14238    [<user>, <user>, iran, has, the, 2, n, biggest...\n","14239    [or, maybe, those, were, not, meant, to, be, h...\n","14240            [good, morning, ados, black, women, only]\n","14241    [the, main, reason, you, do, not, come, here, ...\n","Name: text, Length: 14242, dtype: object\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gx8Oic4ERh1c"},"source":["# Run these"]},{"cell_type":"code","metadata":{"id":"xIGt-krcw6CF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615568161883,"user_tz":-330,"elapsed":1249,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}},"outputId":"90f16968-f214-4f40-8ceb-468e92889b72"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.stem.porter import *\n","import string\n","import re\n","\n","stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n","\n","other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n","stopwords.extend(other_exclusions)\n","\n","stemmer = PorterStemmer()\n","\n","\n","def preprocess(text_string):\n","    \"\"\"\n","    Accepts a text string and replaces:\n","    1) urls with URLHERE\n","    2) lots of whitespace with one instance\n","    3) mentions with MENTIONHERE\n","\n","    This allows us to get standardized counts of urls and mentions\n","    Without caring about specific people mentioned\n","    \"\"\"\n","    space_pattern = '\\s+'\n","    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n","        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","    mention_regex = '@[\\w\\-]+'\n","    parsed_text = re.sub(space_pattern, ' ', text_string)\n","    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n","    parsed_text = re.sub(mention_regex, '', parsed_text)\n","    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n","    return parsed_text\n","\n","def tokenize(tweet):\n","    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n","    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n","    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n","    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n","    tokens = [stemmer.stem(t) for t in tweet.split()]\n","    return tokens\n","\n","def basic_tokenize(tweet):\n","    \"\"\"Same as tokenize but without the stemming\"\"\"\n","    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n","    return tweet.split()\n","\n","vectorizer = TfidfVectorizer(\n","    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n","    tokenizer=tokenize,\n","    preprocessor=preprocess,\n","    ngram_range=(1, 3),\n","    stop_words=stopwords, #We do better when we keep stopwords\n","    use_idf=True,\n","    smooth_idf=False,\n","    norm=None, #Applies l2 norm smoothing\n","    decode_error='replace',\n","    max_features=10000,\n","    min_df=5,\n","    max_df=0.501\n","    )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4uy2yQ6mei2E"},"source":["def random_seed(seed_value, use_cuda):\n","    np.random.seed(seed_value)  \n","    torch.manual_seed(seed_value)  \n","    random.seed(seed_value)\n","    if use_cuda:\n","        torch.cuda.manual_seed(seed_value)\n","        torch.cuda.manual_seed_all(seed_value)  \n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","random_seed(RANDOM_SEED, True)\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9FCCRy1ZFus"},"source":["def split_stratified_into_train_val_test(df_input, stratify_colname='y',\n","                                         frac_train=0.6, frac_val=0.15, frac_test=0.25,\n","                                         random_state=None):\n","    if frac_train + frac_val + frac_test != 1.0:\n","        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \\\n","                         (frac_train, frac_val, frac_test))\n","\n","    if stratify_colname not in df_input.columns:\n","        raise ValueError('%s is not a column in the dataframe' % (stratify_colname))\n","\n","    X = df_input # Contains all columns.\n","    y = df_input[[stratify_colname]] # Dataframe of just the column on which to stratify.\n","\n","    # Split original dataframe into train and temp dataframes.\n","    df_train, df_temp, y_train, y_temp = train_test_split(X,\n","                                                          y,\n","                                                          stratify=y,\n","                                                          test_size=(1.0 - frac_train),\n","                                                          random_state=random_state)\n","\n","    # Split the temp dataframe into val and test dataframes.\n","    relative_frac_test = frac_test / (frac_val + frac_test)\n","    df_val, df_test, y_val, y_test = train_test_split(df_temp,\n","                                                      y_temp,\n","                                                      stratify=y_temp,\n","                                                      test_size=relative_frac_test,\n","                                                      random_state=random_state)\n","\n","    assert len(df_input) == len(df_train) + len(df_val) + len(df_test)\n","    return df_train, df_val, df_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4oavYzbNaV4e"},"source":["#Now get other features\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n","from textstat.textstat import *\n","\n","sentiment_analyzer = VS()\n","\n","def count_twitter_objs(text_string):\n","    \"\"\"\n","    Accepts a text string and replaces:\n","    1) urls with URLHERE\n","    2) lots of whitespace with one instance\n","    3) mentions with MENTIONHERE\n","    4) hashtags with HASHTAGHERE\n","\n","    This allows us to get standardized counts of urls and mentions\n","    Without caring about specific people mentioned.\n","    \n","    Returns counts of urls, mentions, and hashtags.\n","    \"\"\"\n","    space_pattern = '\\s+'\n","    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n","        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","    mention_regex = '@[\\w\\-]+'\n","    hashtag_regex = '#[\\w\\-]+'\n","    parsed_text = re.sub(space_pattern, ' ', text_string)\n","    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n","    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n","    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n","    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n","\n","def other_features(tweet):\n","    \"\"\"This function takes a string and returns a list of features.\n","    These include Sentiment scores, Text and Readability scores,\n","    as well as Twitter specific features\"\"\"\n","    ##SENTIMENT\n","    sentiment = sentiment_analyzer.polarity_scores(tweet)\n","    \n","    words = preprocess(tweet) #Get text only\n","    \n","    syllables = textstat.syllable_count(words) #count syllables in words\n","    num_chars = sum(len(w) for w in words) #num chars in words\n","    num_chars_total = len(tweet)\n","    num_terms = len(tweet.split())\n","    num_words = len(words.split())\n","    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n","    num_unique_terms = len(set(words.split()))\n","    \n","    ###Modified FK grade, where avg words per sentence is just num words/1\n","    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n","    ##Modified FRE score, where sentence fixed to 1\n","    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n","    \n","    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n","    retweet = 0\n","    if \"rt\" in words:\n","        retweet = 1\n","    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n","                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n","                twitter_objs[2], twitter_objs[1],\n","                twitter_objs[0], retweet]\n","    #features = pandas.DataFrame(features)\n","    return features\n","\n","def get_feature_array(tweets):\n","    feats=[]\n","    for t in tweets:\n","        feats.append(other_features(t))\n","    return np.array(feats)\n","\n","other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n","                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \"vader compound\", \\\n","                        \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]\n","\n","#Finally get a list of variable names\n","variables = ['']*len(vocab)\n","for k,v in vocab.items():\n","    variables[v] = k\n","\n","pos_variables = ['']*len(pos_vocab)\n","for k,v in pos_vocab.items():\n","    pos_variables[v] = k\n","\n","feature_names = variables+pos_variables+other_features_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j7YVnLcLn_ZX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615569890721,"user_tz":-330,"elapsed":1095,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}},"outputId":"234b00dd-d955-4adb-ef44-601c807a8a2e"},"source":["class_names = ['Caucasian', 'Hispanic', 'Indian', 'African']\n","parameter = 'Race'\n","df = pd.read_csv('data/annotators_race.csv')\n","df_train, df_val, df_test = split_stratified_into_train_val_test(df, stratify_colname=parameter,\n","                                         frac_train=0.8, frac_val=0.1, frac_test=0.1,\n","                                         random_state=42)\n","len(df_train), len(df_val), len(df_test)\n","\n","try:\n","    with open('results_individual_ml.pkl', 'rb') as f:\n","        results = pickle.load(f)\n","except:\n","    results = {}\n","    with open('results_individual_ml.pkl', 'wb') as f:\n","        pickle.dump(results, f)\n","print(results)\n","if parameter not in results: results[parameter] = {}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MNy8I_xzamWi"},"source":["for c1 in class_names:\n","    if c1 in results[parameter]: continue\n","    else: results[parameter][c1] = {}\n","    for c2 in class_names:\n","        train_df = df_train[df_train[parameter] == c1]\n","        test_df = df_test[df_test[parameter] == c2]\n","\n","        vectorizer = TfidfVectorizer(\n","            #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n","            tokenizer=tokenize,\n","            preprocessor=preprocess,\n","            ngram_range=(1, 3),\n","            stop_words=stopwords, #We do better when we keep stopwords\n","            use_idf=True,\n","            smooth_idf=False,\n","            norm=None, #Applies l2 norm smoothing\n","            decode_error='replace',\n","            max_features=10000,\n","            min_df=5,\n","            max_df=0.501\n","            )\n","\n","        #Construct tfidf matrix and get relevant scores\n","        tweets_train = list(train_df['text'])\n","        tweets_test = list(test_df['text'])\n","        tfidf_transform = vectorizer.fit(tweets_train)\n","        vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n","        idf_vals = vectorizer.idf_\n","        idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores\n","\n","        tweet_tags_train = []\n","        for t in tweets_train:\n","            tokens = basic_tokenize(preprocess(t))\n","            tags = nltk.pos_tag(tokens)\n","            tag_list = [x[1] for x in tags]\n","            #for i in range(0, len(tokens)):\n","            tag_str = \" \".join(tag_list)\n","            tweet_tags_train.append(tag_str)\n","\n","        tweet_tags_test = []\n","        for t in tweets_test:\n","            tokens = basic_tokenize(preprocess(t))\n","            tags = nltk.pos_tag(tokens)\n","            tag_list = [x[1] for x in tags]\n","            #for i in range(0, len(tokens)):\n","            tag_str = \" \".join(tag_list)\n","            tweet_tags_test.append(tag_str)\n","\n","        #We can use the TFIDF vectorizer to get a token matrix for the POS tags\n","        pos_vectorizer = TfidfVectorizer(\n","            #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n","            tokenizer=None,\n","            lowercase=False,\n","            preprocessor=None,\n","            ngram_range=(1, 3),\n","            stop_words=None, #We do better when we keep stopwords\n","            use_idf=False,\n","            smooth_idf=False,\n","            norm=None, #Applies l2 norm smoothing\n","            decode_error='replace',\n","            max_features=5000,\n","            min_df=5,\n","            max_df=0.501,\n","            )\n","\n","        pos_transform = pos_vectorizer.fit(pd.Series(tweet_tags_train))\n","        pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}\n","\n","        tfidf_train = tfidf_transform.transform(tweets_train).toarray()\n","        tfidf_test = tfidf_transform.transform(tweets_test).toarray()\n","\n","        pos_train = pos_transform.transform(pd.Series(tweet_tags_train)).toarray()\n","        pos_test = pos_transform.transform(pd.Series(tweet_tags_test)).toarray()\n","\n","        feats_train = get_feature_array(tweets_train)\n","        feats_test = get_feature_array(tweets_test)\n","\n","        M_train = np.concatenate([tfidf_train,pos_train,feats_train],axis=1)\n","        M_test = np.concatenate([tfidf_test,pos_test,feats_test],axis=1)\n","\n","        X_train = pd.DataFrame(M_train)\n","        y_train = train_df['label'].astype(int)\n","        X_test = pd.DataFrame(M_test)\n","        y_test = test_df['label'].astype(int)\n","\n","        from sklearn.linear_model import LogisticRegression\n","        from sklearn.feature_selection import SelectFromModel\n","        from sklearn.metrics import classification_report\n","        from sklearn.svm import LinearSVC\n","\n","        select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l2\",C=0.01))\n","        transform = select.fit(X_train,y_train)\n","\n","        X_train_ = transform.transform(X_train)\n","        X_test_ = transform.transform(X_test)\n","\n","        model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_train_, y_train)\n","\n","        y_preds_train = model.predict(X_train_)\n","        y_preds_test = model.predict(X_test_)\n","\n","        f1 = f1_score(y_test, y_preds_test, average='macro')\n","        acc = accuracy_score(y_test, y_preds_test)\n","\n","        if c1 not in results[parameter]: results[parameter][c1] = {}\n","\n","        results[parameter][c1][c2] = {'f1': f1, 'acc':acc}\n","        with open('results_individual_ml.pkl', 'wb') as f:\n","            pickle.dump(results, f)\n","        print(c1, c2, f1, acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKPMl_fWKZyn"},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"," \n","def get_predicted(preds):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    return pred_flat\n"," \n","def evaluate(test_dataloader, model):\n","    model.eval()\n","    y_preds, y_test = np.array([]), np.array([])\n","\n","    for batch in test_dataloader:\n","        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device).long()\n","        with torch.no_grad():        \n","            ypred = model(b_input_ids, b_input_mask)\n","        ypred = ypred[0].cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        y_preds = np.hstack((y_preds, get_predicted(ypred)))\n","        y_test = np.hstack((y_test, label_ids))\n","\n","    macro_f1 = f1_score(y_test, y_preds, average='macro')\n","    report = classification_report(y_test, y_preds)\n","    print(report)\n","    return macro_f1, y_preds, y_test\n"," \n","def train(training_dataloader, validation_dataloader, model, filepath = None, weights = None, learning_rate = 2e-5, epochs = 1, print_every = 10):\n","    total_steps = len(training_dataloader) * epochs\n","    no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps = 1e-8)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps)\n","    \n","    best_weighted_f1 = 0\n","    best_model = None\n","    # current_epoch, best_weighted_f1 = load_metrics(filepath, model, optimizer)\n","    if weights == None:\n","        criterion = nn.CrossEntropyLoss()\n","    else:\n","        criterion = nn.CrossEntropyLoss(weight=weights)\n","    for epoch_i in tqdm(range(0, epochs)):\n","        model.train()\n","        for step, batch in enumerate(training_dataloader):\n","            b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device).long()\n","            \n","            outputs = model(b_input_ids, b_input_mask)\n","            loss = criterion(outputs[0], b_labels)\n"," \n","            if step%print_every == 0:\n","                print(loss.item())\n"," \n","            optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","            scheduler.step()\n"," \n","        print('### Validation Set Stats')\n","        weighted_f1, ypred, ytest = evaluate(validation_dataloader, model)\n","        print(\"  Macro F1: {0:.3f}\".format(weighted_f1))\n","        if weighted_f1 > best_weighted_f1:\n","            best_weighted_f1 = weighted_f1\n","            best_model = model\n","            # save_metrics(filepath, epoch_i, model, optimizer, weighted_f1)\n","        \n","    return best_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npT6gvzmOJV3","executionInfo":{"status":"ok","timestamp":1615567765312,"user_tz":-330,"elapsed":1228,"user":{"displayName":"Kushal Kedia","photoUrl":"","userId":"15395307286796798856"}},"outputId":"4e574311-baf0-4a42-daeb-139f203a239f"},"source":["class_names = ['Caucasian', 'Hispanic', 'Indian', 'African']\n","parameter = 'Race'\n","df = pd.read_csv('data/annotators_race.csv')\n","df_train, df_val, df_test = split_stratified_into_train_val_test(df, stratify_colname=parameter,\n","                                         frac_train=0.8, frac_val=0.1, frac_test=0.1,\n","                                         random_state=42)\n","# len(df_train), len(df_val), len(df_test)\n","\n","# try:\n","#     with open('results_individual.pkl', 'rb') as f:\n","#         results = pickle.load(f)\n","# except:\n","results = {}\n","with open('results_individual.pkl', 'wb') as f:\n","    pickle.dump(results, f)\n","print(results)\n","if parameter not in results: results[parameter] = {}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_kuG3xPN9ed","outputId":"ecdc3c03-8810-416a-f82a-cf7c90ed70eb"},"source":["for c1 in class_names:\n","    if c1 in results[parameter]: continue\n","    else: results[parameter][c1] = {}\n","    train_df = df_train[df_train[parameter] == c1]\n","    val_df = df_val[df_val[parameter] == c1]\n","    train_data, val_data = Dataset(train_df), Dataset(val_df)\n","    model = SC_weighted_BERT(model_path).to(device)\n","    model = train(train_data.DataLoader, val_data.DataLoader, model, None, epochs = 5)\n","    for c2 in class_names:\n","        test_df = df_test[df_test[parameter] == c2]\n","        test_data = Dataset(test_df)\n","        f1, ypreds, ytest = evaluate(test_data.DataLoader, model)\n","        acc = accuracy_score(ytest, ypreds)\n","        results[parameter][c1][c2] = {'f1': f1, 'acc':acc}\n","    with open('results_individual.pkl', 'rb') as f:\n","        results = pickle.load(f)\n","    with open('results_individual.pkl', 'wb') as f:\n","        pickle.dump(results, f)\n","    print(c1, results[c1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5031\n","629\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1.1332472562789917\n","1.032253384590149\n","1.0149624347686768\n","1.017764925956726\n","0.8575710654258728\n","0.7544137239456177\n","0.8850621581077576\n","0.8717197179794312\n","0.8048531413078308\n","0.7126165628433228\n","0.7826817631721497\n","0.7599096298217773\n","0.6364507675170898\n","0.8325504064559937\n","0.799396276473999\n","0.6170777678489685\n","### Validation Set Stats\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YWsllqy-Q9N8"},"source":[""],"execution_count":null,"outputs":[]}]}